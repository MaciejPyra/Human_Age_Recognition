{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import #\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import hashlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project directory definition #\n",
    "\n",
    "project_dir = os.path.abspath('').rsplit(os.path.sep, 1)[0].replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading and modifying the table with age assignment from the initial set #\n",
    "\n",
    "labels_dir = project_dir + '/data/external/labels.xlsx'\n",
    "labels1 = pd.read_excel(labels_dir)\n",
    "labels_age = labels1.loc[labels1.age.isin(['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the age table for images from the initial set #\n",
    "\n",
    "labels_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating bar chart of age category frequencies in the initial set #\n",
    "\n",
    "fig1_dir = project_dir + '/Figures/figure1.jpg'\n",
    "plt.figure(figsize=[8, 5])\n",
    "splot = sns.countplot(labels_age['age'], order=['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)'])\n",
    "plt.xlabel('wiek', fontsize=12)\n",
    "plt.ylabel('liczebność', fontsize=12)\n",
    "for p in splot.patches:\n",
    "    splot.annotate(p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "plt.savefig(fig1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the number of photos not assigned to any age categories #\n",
    "\n",
    "labels1.loc[~labels1.age.isin(['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']), :].age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating gender distribution bar chart for data in the initial set #\n",
    "\n",
    "fig2_dir = project_dir + '/Figures/figure2.jpg'\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "splot = sns.countplot(labels1['gender'])\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels[0] = 'Kobiety'\n",
    "labels[1] = 'Mężczyźni'\n",
    "labels[2] = 'Płeć nieznana'\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Płeć', fontsize=12)\n",
    "ax.set_ylabel('Liczebność', fontsize=12)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.savefig(fig2_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating gender distribution bar chart depending on the age category for data in the initial set #\n",
    "\n",
    "fig3_dir = project_dir + '/Figures/figure3.jpg'\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='age',hue='gender',data=labels_age, order=['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)'])\n",
    "plt.xlabel('wiek', fontsize=12)\n",
    "plt.ylabel('liczebność', fontsize=12)\n",
    "plt.legend(['Kobiety', 'Mężczyźni', 'Płeć nieznana'])\n",
    "\n",
    "plt.savefig(fig3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating gender distribution bar chart depending on the age category for data in the initial set #\n",
    "\n",
    "fig4_dir = project_dir + '/Figures/figure4.jpg'\n",
    "age_category = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(11, 23))\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for category in age_category:\n",
    "    counts = labels_age.loc[labels_age.age.isin([category]), 'gender'].value_counts()\n",
    "    female = list(counts.index).index('f')\n",
    "    male = list(counts.index).index('m')\n",
    "    if len(list(counts.index)) == 3:\n",
    "        unknown = list(counts.index).index('u')\n",
    "        counts_new = pd.Series(data=[counts[female], counts[male], counts[unknown]], index=['f', 'm', 'u'])\n",
    "    else:\n",
    "        counts_new = pd.Series(data=[counts[female], counts[male]], index=['f', 'm'])\n",
    "    barplot = ax[i, j].bar(counts_new.index, counts_new.values)\n",
    "    labels = [item.get_text() for item in ax[i, j].get_xticklabels()]\n",
    "    labels[0] = 'Kobiety'\n",
    "    barplot[0].set_color('blueviolet')\n",
    "    labels[1] = 'Mężczyźni'\n",
    "    barplot[1].set_color('darkorange')\n",
    "    if len(list(counts_new.index)) == 3:\n",
    "        labels[2] = 'Płeć nieznana'\n",
    "        barplot[2].set_color('forestgreen')\n",
    "    ax[i, j].set_xticklabels(labels)\n",
    "    ax[i, j].set_xlabel('Płeć', fontsize=12)\n",
    "    ax[i, j].set_ylabel('Liczebność', fontsize=12)\n",
    "    ax[i, j].set_title(\"Kategoria '\" + category + \"'\")\n",
    "    \n",
    "    for bar in barplot.patches:\n",
    "        ax[i, j].annotate(bar.get_height(), (bar.get_x() + bar.get_width() / 2., bar.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    " \n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        j = 0\n",
    "        i += 1\n",
    "        \n",
    "plt.savefig(fig4_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading and modifying the age table for images #\n",
    "\n",
    "labels = pd.read_excel(labels_dir)\n",
    "\n",
    "labels.loc[labels.age == '(0, 2)', 'age'] = '(0, 6)'\n",
    "labels.loc[labels.age == '(4, 6)', 'age'] = '(0, 6)'\n",
    "labels.loc[labels.age == '(8, 12)', 'age'] = '(7, 20)'\n",
    "labels.loc[labels.age == '(15, 20)', 'age'] = '(7, 20)'\n",
    "labels.loc[labels.age == '(8, 23)', 'age'] = '(7, 20)'\n",
    "labels.loc[labels.age == '(25, 32)', 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == '(38, 43)', 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == '(60, 100)', 'age'] = '(60+)'\n",
    "labels.loc[labels.age == '(48, 53)', 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == '(38, 48)', 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 2, 'age'] = '(0, 6)'\n",
    "labels.loc[labels.age == 3, 'age'] = '(0, 6)'\n",
    "labels.loc[labels.age == 13, 'age'] = '(7, 20)'\n",
    "labels.loc[labels.age == 22, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 23, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 29, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 32, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 33, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 34, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 35, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 36, 'age'] = '(21, 37)'\n",
    "labels.loc[labels.age == 42, 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 45, 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 46, 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 55, 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 56, 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 57, 'age'] = '(38, 59)'\n",
    "labels.loc[labels.age == 58, 'age'] = '(38, 59)'\n",
    "\n",
    "\n",
    "labels.loc[labels.age == '(0, 6)', 'age'] = 0\n",
    "labels.loc[labels.age == '(7, 20)', 'age'] = 1\n",
    "labels.loc[labels.age == '(21, 37)', 'age'] = 2\n",
    "labels.loc[labels.age == '(38, 59)', 'age'] = 3\n",
    "labels.loc[labels.age == '(60+)', 'age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the age table #\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controling the number of age categories in the dataset #\n",
    "\n",
    "print('Number of images in the dataset:', len(np.unique(labels.index)))\n",
    "print('Age categories frequencies in the dataset:', labels['age'].value_counts(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of the age categories frequencies #\n",
    "\n",
    "print(sns.countplot(labels['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading photos from the initial repository 'external' and saving them in folder 'raw' #\n",
    "\n",
    "file_dir = project_dir + '/data/external'\n",
    "file_dir1 = project_dir + '/data/raw'\n",
    "filenames = os.listdir(file_dir)\n",
    "images_number = len(np.unique(labels.index)) # number of images in the initial file\n",
    "w = 0\n",
    "for filename in filenames:\n",
    "    images = os.listdir(file_dir + '/' + filename)\n",
    "    for i in images:\n",
    "        if i.endswith('.jpg'):\n",
    "            image_name = i.split(sep='.', maxsplit=1)[1]\n",
    "\n",
    "            img_array = plt.imread(os.path.join(file_dir, filename, i))\n",
    "            \n",
    "            im = Image.fromarray(img_array)\n",
    "            im.save(file_dir1 + '/' + filename + '_' + image_name)\n",
    "            \n",
    "            if w > 5:\n",
    "                if w % np.round(0.1 * images_number, 0) == 0:\n",
    "                    progress = np.round(w / images_number, 1) * 100\n",
    "                    print('Progress: ' + str(progress) + '%')\n",
    "                    \n",
    "            w += 1\n",
    "            \n",
    "            if w == images_number:\n",
    "                print('Progress: 100.0%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the photos are not corrupted - displaying the number of damaged photos in the repository #\n",
    "\n",
    "images = os.listdir(file_dir1)\n",
    "corrupted_images = []\n",
    "w = 0\n",
    "for image in images:\n",
    "    \n",
    "    try:\n",
    "      img = Image.open(os.path.join(file_dir1, image))\n",
    "      img.verify()\n",
    "    except (IOError, SyntaxError):\n",
    "      corrupted_images.append(image)\n",
    "    finally:\n",
    "        if w > 5:\n",
    "            if w % np.round(0.1 * images_number, 0) == 0:\n",
    "                progress = np.round(w / images_number, 1) * 100\n",
    "                print('Progress: ' + str(progress) + '%')\n",
    "\n",
    "        w += 1\n",
    "\n",
    "        if w == images_number:\n",
    "            print('Progress: 100.0%')\n",
    "    \n",
    "print('Corrupted images number:', len(corrupted_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate images - throwing duplicate names into the list #\n",
    "\n",
    "duplicates = []\n",
    "hash_keys = {}\n",
    "w = 0\n",
    "\n",
    "for index, filename in enumerate(os.listdir(file_dir1)):\n",
    "    with open(os.path.join(file_dir1, filename), 'rb') as f:\n",
    "        filehash = hashlib.md5(f.read()).hexdigest()\n",
    "        if filehash not in hash_keys:\n",
    "            hash_keys[filehash] = index\n",
    "            \n",
    "        else:\n",
    "            duplicates.append(filename)\n",
    "            \n",
    "        if w > 5:\n",
    "            if w % np.round(0.1 * images_number, 0) == 0:\n",
    "                progress = np.round(w / images_number, 1) * 100\n",
    "                print('Progress: ' + str(progress) + '%')\n",
    "                \n",
    "        w += 1\n",
    "        \n",
    "        if w == images_number:\n",
    "            print('Progress: 100.0%')\n",
    "\n",
    "print('Duplicates number in the initial dataset:', len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate photos from the age table #\n",
    "\n",
    "labels_new = labels.loc[~labels.original_image_full_name.isin(duplicates), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the age table after removing duplicates #\n",
    "\n",
    "labels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying age categories frequencies after removing duplicates #\n",
    "\n",
    "labels_new.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying genders frequencies in age categories after removing duplicates #\n",
    "\n",
    "labels_new1 = labels_new.groupby('age')\n",
    "labels_new1.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading images from the repository to check their resolution #\n",
    "\n",
    "images = os.listdir(file_dir1)\n",
    "dataset_resolution = []\n",
    "w = 0\n",
    "for i in images:     \n",
    "    if w > 5:\n",
    "        if w % np.round(0.1 * images_number, 0) == 0:\n",
    "            progress = np.round(w / images_number, 1) * 100\n",
    "            print('Progress: ' + str(progress) + '%')\n",
    "    w += 1\n",
    "\n",
    "    if w == images_number:\n",
    "        print('Progress: 100.0%')\n",
    "\n",
    "    if i in duplicates:\n",
    "        continue\n",
    "\n",
    "    img_array = plt.imread(os.path.join(file_dir1, i))\n",
    "    dataset_resolution.append(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a graph with image resolutions and selecting the appropriate resolution for resizing #\n",
    "\n",
    "fig5_dir = project_dir + '/Figures/figure5.jpg'\n",
    "heights = []\n",
    "widths = []\n",
    "ratio = []\n",
    "for image in dataset_resolution:\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    aspect_ratio = round(image.shape[1] / image.shape[0], 2) \n",
    "    heights.append(height)\n",
    "    widths.append(width)\n",
    "    ratio.append(aspect_ratio)\n",
    "    \n",
    "resolution_table = pd.DataFrame({'height': heights, 'width': widths, 'aspect_ratio': ratio})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(resolution_table.width, resolution_table.height, color='blue', alpha=0.5, s=resolution_table.aspect_ratio*100, label='Format konkretnego zdjęcia')\n",
    "x = np.linspace(*ax.get_xlim())\n",
    "ax.plot(x, x, 'red', linewidth=2, label='Format obrazu (Aspect ratio) = 1')\n",
    "ax.set_title('Rozdzielczość zdjęć', fontsize=16)\n",
    "ax.set_xlabel('Szerokość', fontsize=12)\n",
    "ax.set_ylabel('Wysokość', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "plt.savefig(fig5_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating image examples of using the zero padding method #\n",
    "\n",
    "fig6_dir = project_dir + '/Figures/figure6.jpg'\n",
    "img_array1 = dataset_resolution[202]\n",
    "img_array2 = dataset_resolution[1111]\n",
    "\n",
    "old_image_height1, old_image_width1, channels1 = img_array1.shape\n",
    "old_image_height2, old_image_width2, channels2 = img_array2.shape\n",
    "\n",
    "if old_image_height1 != old_image_width1:\n",
    "\n",
    "    new_image_width1 = max(old_image_height1, old_image_width1)\n",
    "    new_image_height1 = max(old_image_height1, old_image_width1)\n",
    "    color1 = (0,0,0)\n",
    "    result1 = np.full((new_image_height1, new_image_width1, 3), color1, dtype=np.uint8)\n",
    "\n",
    "    x_center1 = (new_image_width1 - old_image_width1) // 2\n",
    "    y_center1 = (new_image_height1 - old_image_height1) // 2\n",
    "\n",
    "    result1[y_center1:y_center1+old_image_height1, \n",
    "           x_center1:x_center1+old_image_width1] = img_array1\n",
    "\n",
    "else:\n",
    "\n",
    "    result1 = img_array1\n",
    "    \n",
    "if old_image_height2 != old_image_width2:\n",
    "\n",
    "    new_image_width2 = max(old_image_height2, old_image_width2)\n",
    "    new_image_height2 = max(old_image_height2, old_image_width2)\n",
    "    color2 = (0,0,0)\n",
    "    result2 = np.full((new_image_height2, new_image_width2, 3), color2, dtype=np.uint8)\n",
    "\n",
    "    x_center2 = (new_image_width2 - old_image_width2) // 2\n",
    "    y_center2 = (new_image_height2 - old_image_height2) // 2\n",
    "\n",
    "    result2[y_center2:y_center2+old_image_height2, \n",
    "           x_center2:x_center2+old_image_width2] = img_array2\n",
    "\n",
    "else:\n",
    "\n",
    "    result2 = img_array2\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(img_array1)\n",
    "plt.title('Zdjęcie oryginalne')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(result1)\n",
    "plt.title('Zdjęcie po modyfikacji')\n",
    "    \n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(img_array2)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(result2)\n",
    "plt.savefig(fig6_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading images from the repository, padding and resizing them  #\n",
    "\n",
    "images = os.listdir(file_dir1)\n",
    "dataset = []\n",
    "w = 0\n",
    "    \n",
    "for i in images:\n",
    "    if w > 5:\n",
    "        if w % np.round(0.1 * images_number, 0) == 0:\n",
    "            progress = np.round(w / images_number, 1) * 100\n",
    "            print('Progress: ' + str(progress) + '%')\n",
    "    w += 1\n",
    "    \n",
    "    if w == images_number:\n",
    "        print('Progress: 100.0%')\n",
    "    \n",
    "    if i in duplicates:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        label_y = labels_new.loc[labels_new.original_image_full_name == i, 'age'].iloc[0]\n",
    "    except:\n",
    "        continue\n",
    "    else:\n",
    "\n",
    "        img_array = plt.imread(os.path.join(file_dir1, i))\n",
    "\n",
    "        old_image_height, old_image_width, channels = img_array.shape\n",
    "\n",
    "        if old_image_height != old_image_width:\n",
    "\n",
    "            new_image_width = max(old_image_height, old_image_width)\n",
    "            new_image_height = max(old_image_height, old_image_width)\n",
    "            color = (0,0,0)\n",
    "            result = np.full((new_image_height, new_image_width, 3), color, dtype=np.uint8)\n",
    "\n",
    "            x_center = (new_image_width - old_image_width) // 2\n",
    "            y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "            result[y_center:y_center+old_image_height, \n",
    "                   x_center:x_center+old_image_width] = img_array\n",
    "\n",
    "        else:\n",
    "\n",
    "            result = img_array\n",
    "\n",
    "        img_array = np.array(Image.fromarray(result).resize((256, 256)))\n",
    "\n",
    "        dataset.append([img_array, label_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the number of images after resizing size #\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving photos to folders with age categories in the repository for quality control of assigning photos to categories #\n",
    "\n",
    "file_dir2 = project_dir + '/data/interim'\n",
    "image_number_new = len(dataset)\n",
    "w = 0\n",
    "for z in range(5):\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][1] == z:\n",
    "            im = Image.fromarray(dataset[i][0])\n",
    "            im.save(file_dir2 + '/Category_' + str(z) + '/zdjecie_' + str(i) + '.jpeg')\n",
    "            \n",
    "            if w > 5:\n",
    "                if w % np.round(0.1 * image_number_new, 0) == 0:\n",
    "                    progress = np.round(w / image_number_new, 1) * 100\n",
    "                    print('Progress: ' + str(progress) + '%')\n",
    "            w += 1\n",
    "\n",
    "            if w == image_number_new:\n",
    "                print('Progress: 100.0%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image examples from each category:\n",
    "# - 0 -> Infant and preschool children (0-6 years)\n",
    "# - 1 -> Early school age children and adolescents (7-20 years)\n",
    "# - 2 -> Adults in the mobile working age (21-37 years)\n",
    "# - 3 -> Middle aged adults (38-59 years)\n",
    "# - 4 -> Adults over 60 (seniors)\n",
    "\n",
    "fig7_dir = project_dir + '/Figures/figure7.jpg'\n",
    "plt.figure(figsize=[15, 15])\n",
    "\n",
    "Category_0 = list(zip ([1, 6, 11, 16, 21], [804, 790, 2971, 6264, 8351]))\n",
    "Category_1 = list(zip ([2, 7, 12, 17, 22], [2097, 3569, 4138, 12363, 13466]))\n",
    "Category_2 = list(zip ([3, 8, 13, 18, 23], [642, 10, 3238, 692, 2371]))\n",
    "Category_3 = list(zip ([4, 9, 14, 19, 24], [15178, 11936, 458, 1768, 18275]))\n",
    "Category_4 = list(zip ([5, 10, 15, 20, 25], [17687, 18298, 12506, 5241, 17962]))\n",
    "\n",
    "for i in Category_0:\n",
    "    plt.subplot(5, 5, i[0])\n",
    "    plt.imshow(dataset[i[1]][0])\n",
    "    if i[0] == 1:\n",
    "        plt.title('Kategoria 0')\n",
    "   \n",
    "for i in Category_1:\n",
    "    plt.subplot(5, 5, i[0])\n",
    "    plt.imshow(dataset[i[1]][0])\n",
    "    if i[0] == 2:\n",
    "        plt.title('Kategoria 1')\n",
    "    \n",
    "for i in Category_2:\n",
    "    plt.subplot(5, 5, i[0])\n",
    "    plt.imshow(dataset[i[1]][0])\n",
    "    if i[0] == 3:\n",
    "        plt.title('Kategoria 2')\n",
    "\n",
    "for i in Category_3:\n",
    "    plt.subplot(5, 5, i[0])\n",
    "    plt.imshow(dataset[i[1]][0])\n",
    "    if i[0] == 4:\n",
    "        plt.title('Kategoria 3')\n",
    "\n",
    "for i in Category_4:\n",
    "    plt.subplot(5, 5, i[0])\n",
    "    plt.imshow(dataset[i[1]][0])\n",
    "    if i[0] == 5:\n",
    "        plt.title('Kategoria 4')\n",
    "    \n",
    "plt.savefig(fig7_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images and their age categories in separate lists #\n",
    "\n",
    "feature_list = []\n",
    "label_list = []\n",
    "for feature, label in dataset:\n",
    "    feature_list.append(feature)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a list with images and a list with their age categories in a numpy array #\n",
    "\n",
    "X_dataset = np.array(feature_list)\n",
    "Y_dataset = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the numpy arrays dimensions respectively for the image set and for their age category #\n",
    "\n",
    "print(X_dataset.shape)\n",
    "print(Y_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division of the set into training and test sets #\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(X_dataset, Y_dataset, test_size=0.4, shuffle=True, stratify=Y_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division of the test set into validation and test sets #\n",
    "\n",
    "validation_X, test_X, validation_Y, test_Y = train_test_split(val_X, val_Y, test_size=0.5, shuffle=True, stratify=val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the dimensions of the created sets #\n",
    "\n",
    "print('Training set X:', train_X.shape)\n",
    "print('Training set Y:', train_Y.shape)\n",
    "print('Validation set X:', validation_X.shape)\n",
    "print('Validation set Y:', validation_Y.shape)\n",
    "print('Test set X:', test_X.shape)\n",
    "print('Test set Y:', test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving training, validation and test sets with their age categories in the 'processed' repository #\n",
    "\n",
    "file_dir3 = project_dir + '/data/processed'\n",
    "names_train = []\n",
    "names_validation = []\n",
    "names_test = []\n",
    "w = 0\n",
    "for i in range(train_X.shape[0]):\n",
    "    w += 1\n",
    "    im = Image.fromarray(train_X[i].astype(np.uint8))\n",
    "    im.save(file_dir3 + '/train/photos/zdjecie_' + str(w) + '_kategoria_' + str(train_Y[i]) + '.jpg')\n",
    "    image_name = 'zdjecie_' + str(w)\n",
    "    names_train.append(image_name)\n",
    "\n",
    "table1 = pd.DataFrame({'original_image': names_train, 'labels': train_Y})\n",
    "table1.to_csv(file_dir3 + '/train/labels.csv', index=False)\n",
    "print('Training set saved!')\n",
    "\n",
    "for i in range(validation_X.shape[0]):\n",
    "    w += 1\n",
    "    im = Image.fromarray(validation_X[i].astype(np.uint8))\n",
    "    im.save(file_dir3 + '/validation/photos/zdjecie_' + str(w) + '_kategoria_' + str(validation_Y[i]) +'.jpg')\n",
    "    image_name = 'zdjecie_' + str(w)\n",
    "    names_validation.append(image_name)\n",
    "    \n",
    "table2 = pd.DataFrame({'original_image': names_validation, 'labels': validation_Y})\n",
    "table2.to_csv(file_dir3 + '/validation/labels.csv', index=False)\n",
    "print('Validation set saved!')\n",
    "\n",
    "for i in range(test_X.shape[0]):\n",
    "    w += 1\n",
    "    im = Image.fromarray(test_X[i].astype(np.uint8))\n",
    "    im.save(file_dir3 + '/test/photos/zdjecie_' + str(w) + '_kategoria_' + str(test_Y[i]) +'.jpg')\n",
    "    image_name = 'zdjecie_' + str(w)\n",
    "    names_test.append(image_name)\n",
    "                       \n",
    "table3 = pd.DataFrame({'original_image': names_test, 'labels': test_Y})\n",
    "table3.to_csv(file_dir3 + '/test/labels.csv', index=False)\n",
    "print('Test set saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading images into training, validation and test sets from the repository #\n",
    "\n",
    "filenames = os.listdir(file_dir3)\n",
    "for filename in filenames:\n",
    "    dataset_import = []\n",
    "    category_list = []\n",
    "    images = os.listdir(file_dir3 + '/' + filename + '/photos')\n",
    "    for image in images:\n",
    "        img_array = plt.imread(os.path.join(file_dir3, filename, 'photos', image))\n",
    "        category = int(image.split('_', 3)[3].split('.', 1)[0])\n",
    "        dataset_import.append(img_array)\n",
    "        category_list.append(category)\n",
    "    if filename == 'train':\n",
    "        train_X = np.array(dataset_import)\n",
    "        train_Y = np.array(category_list)\n",
    "    if filename == 'validation':\n",
    "        validation_X = np.array(dataset_import)\n",
    "        validation_Y = np.array(category_list)\n",
    "    if filename == 'test':\n",
    "        test_X = np.array(dataset_import)\n",
    "        test_Y = np.array(category_list)\n",
    "    print('Uploaded set ', filename, '!', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing age categories in training, validation and test sets #\n",
    "\n",
    "print('Age categories frequencies in the training set:', Counter(train_Y))\n",
    "print('Age categories frequencies in the validation set:', Counter(validation_Y))\n",
    "print('Age categories frequencies in the test set:', Counter(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of pixel values in images from training, validation and test sets #\n",
    "\n",
    "train_X = train_X / 255.0\n",
    "validation_X = validation_X / 255.0\n",
    "test_X = test_X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model I filepath #\n",
    "\n",
    "model_filepath1 = project_dir + '/models/model_checkpoint1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model I #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier1 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier1.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1.add(Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1.add(Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1.add(Flatten())\n",
    "classifier1.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath1, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture summary I #\n",
    "\n",
    "classifier1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training I #\n",
    "\n",
    "model_plain1 = classifier1.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy I - the best trained on the validation set #\n",
    "\n",
    "Model_I = load_model(model_filepath1)\n",
    "Model_I.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ia filepath #\n",
    "\n",
    "model_filepath1a = project_dir + '/models/model_checkpoint1a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ia #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier1a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier1a.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1a.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1a.add(Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1a.add(Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1a.add(Flatten())\n",
    "classifier1a.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier1a.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath1a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training Ia #\n",
    "\n",
    "model_plain1a = classifier1.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy Ia - the best trained on the validation set #\n",
    "\n",
    "Model_Ia = load_model(model_filepath1a)\n",
    "Model_Ia.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ib filepath #\n",
    "\n",
    "model_filepath1b = project_dir + '/models/model_checkpoint1b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ib #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier1b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier1b.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1b.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1b.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1b.add(Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1b.add(Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier1b.add(Flatten())\n",
    "classifier1b.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier1b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier1b.compile(optimizer=optimizers.Adam(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath1b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training Ib #\n",
    "\n",
    "model_plain1a = classifier1.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy Ib - the best trained on the validation set #\n",
    "\n",
    "Model_Ib = load_model(model_filepath1b)\n",
    "Model_Ib.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model II filepath #\n",
    "\n",
    "model_filepath2 = project_dir + '/models/model_checkpoint2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model II #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier2 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier2.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2.add(Flatten())\n",
    "classifier2.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath2, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training II #\n",
    "\n",
    "model_plain2 = classifier2.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy II - the best trained on the validation set #\n",
    "\n",
    "Model_II = load_model(model_filepath2)\n",
    "Model_II.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IIa filepath #\n",
    "\n",
    "model_filepath2a = project_dir + '/models/model_checkpoint2a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IIa #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier2a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier2a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2a.add(Flatten())\n",
    "classifier2a.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier2a.compile(optimizer=optimizers.Adam(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath2a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IIa #\n",
    "\n",
    "model_plain2a = classifier2a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IIa - the best trained on the validation set #\n",
    "\n",
    "Model_IIa = load_model(model_filepath2a)\n",
    "Model_IIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IIb filepath #\n",
    "\n",
    "model_filepath2b = project_dir + '/models/model_checkpoint2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IIb #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier2b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier2b.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2b.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2b.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier2b.add(Flatten())\n",
    "classifier2b.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier2b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier2b.compile(optimizer=optimizers.Adam(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath2b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IIb #\n",
    "\n",
    "model_plain2b = classifier2a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IIb - the best trained on the validation set #\n",
    "\n",
    "Model_IIb = load_model(model_filepath2b)\n",
    "Model_IIb.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model III filepath #\n",
    "\n",
    "model_filepath3 = project_dir + '/models/model_checkpoint3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model III #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier3 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier3.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier3.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier3.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier3.add(Flatten())\n",
    "classifier3.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath3, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training III - poprawione validation #\n",
    "\n",
    "model_plain3 = classifier3.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy III - the best trained on the validation set #\n",
    "\n",
    "Model_III = load_model(model_filepath3)\n",
    "Model_III.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IIIa filepath #\n",
    "\n",
    "model_filepath3a = project_dir + '/models/model_checkpoint3a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IIIa #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier3a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier3a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier3a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier3a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier3a.add(Flatten())\n",
    "classifier3a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier3a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier3a.compile(optimizer=optimizers.Adam(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath3, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IIIa #\n",
    "\n",
    "model_plain3a = classifier3a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IIIa - the best trained on the validation set #\n",
    "\n",
    "Model_IIIa = load_model(model_filepath3)\n",
    "Model_IIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IV filepath #\n",
    "\n",
    "model_filepath4 = project_dir + '/models/model_checkpoint4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IV #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier4 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier4.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4.add(Flatten())\n",
    "classifier4.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath4, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IV #\n",
    "\n",
    "model_plain4 = classifier4.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IV - the best trained on the validation set #\n",
    "\n",
    "Model_IV = load_model(model_filepath4)\n",
    "Model_IV.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IVa filepath #\n",
    "\n",
    "model_filepath4a = project_dir + '/models/model_checkpoint4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IVa #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier4a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier4a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4a.add(Flatten())\n",
    "classifier4a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier4a.compile(optimizer=optimizers.Adam(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath4a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IVa #\n",
    "\n",
    "model_plain4a = classifier4a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IVa - the best trained on the validation set #\n",
    "\n",
    "Model_IVa = load_model(model_filepath4a)\n",
    "Model_IVa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IVb filepath #\n",
    "\n",
    "model_filepath4b = project_dir + '/models/model_checkpoint4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IVb #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier4b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier4b.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4b.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4b.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier4b.add(Flatten())\n",
    "classifier4b.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier4b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier4b.compile(optimizer=optimizers.Adam(0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath4b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IVb #\n",
    "\n",
    "model_plain4b = classifier4b.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IVb - the best trained on the validation set #\n",
    "\n",
    "Model_IVb = load_model(model_filepath4b)\n",
    "Model_IVb.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model V filepath #\n",
    "\n",
    "model_filepath5 = project_dir + '/models/model_checkpoint5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model V #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier5 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier5.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5.add(Flatten())\n",
    "classifier5.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath5, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training V #\n",
    "\n",
    "model_plain5 = classifier5.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy V - the best trained on the validation set #\n",
    "\n",
    "Model_V = load_model(model_filepath5)\n",
    "Model_V.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Va filepath #\n",
    "\n",
    "model_filepath5a = project_dir + '/models/model_checkpoint5a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Va #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier5a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier5a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5a.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier5a.add(Flatten())\n",
    "classifier5a.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier5a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier5a.compile(optimizer=optimizers.Adam(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath5a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training Va #\n",
    "\n",
    "model_plain5a = classifier5a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy Va - the best trained on the validation set #\n",
    "\n",
    "Model_Va = load_model(model_filepath5a)\n",
    "Model_Va.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VI filepath #\n",
    "\n",
    "model_filepath6 = project_dir + '/models/model_checkpoint6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VI #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier6 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier6.add(Conv2D(16, (7, 7), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6.add(Flatten())\n",
    "classifier6.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier6.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath6, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VI #\n",
    "\n",
    "model_plain6 = classifier6.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VI - the best trained on the validation set #\n",
    "\n",
    "Model_VI = load_model(model_filepath6)\n",
    "Model_VI.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIa filepath #\n",
    "\n",
    "model_filepath6a = project_dir + '/models/model_checkpoint6a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIa #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier6a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier6a.add(Conv2D(16, (7, 7), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6a.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6a.add(Flatten())\n",
    "classifier6a.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier6a.compile(optimizer=optimizers(0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath6a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VIa #\n",
    "\n",
    "model_plain6a = classifier6a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VIa - the best trained on the validation set #\n",
    "\n",
    "Model_VIa = load_model(model_filepath6a)\n",
    "Model_VIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIb filepath #\n",
    "\n",
    "model_filepath6b = project_dir + '/models/model_checkpoint6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIb #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier6b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier6b.add(Conv2D(16, (7, 7), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6b.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6b.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier6b.add(Flatten())\n",
    "classifier6b.add(Dense(units=512, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier6b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier6b.compile(optimizer=optimizers(0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath6b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VIb #\n",
    "\n",
    "model_plain6b = classifier6b.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VIb - the best trained on the validation set #\n",
    "\n",
    "Model_VIb = load_model(model_filepath6b)\n",
    "Model_VIb.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VII filepath #\n",
    "\n",
    "model_filepath7 = project_dir + '/models/model_checkpoint7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VII (double Dropout) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier7 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier7.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7.add(Flatten())\n",
    "classifier7.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(Dropout(0.5))\n",
    "classifier7.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7.add(Dropout(0.5))\n",
    "classifier7.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier7.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath7, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VII #\n",
    "\n",
    "model_dropout7 = classifier7.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VII - the best trained on the validation set #\n",
    "\n",
    "Model_VII = load_model(model_filepath7)\n",
    "Model_VII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIIa filepath #\n",
    "\n",
    "model_filepath7a = project_dir + '/models/model_checkpoint7a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIIa (double Dropout) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier7a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier7a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier7a.add(Flatten())\n",
    "classifier7a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(Dropout(0.5))\n",
    "classifier7a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier7a.add(Dropout(0.5))\n",
    "classifier7a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier7a.compile(optimizer=optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath7a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VIIa #\n",
    "\n",
    "model_dropout7a = classifier7a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VIIa - the best trained on the validation set #\n",
    "\n",
    "Model_VIIa = load_model(model_filepath7a)\n",
    "Model_VIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIII filepath #\n",
    "\n",
    "model_filepath8 = project_dir + '/models/model_checkpoint8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIII (single Dropout) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier8 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier8.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8.add(Flatten())\n",
    "classifier8.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(Dropout(0.5))\n",
    "classifier8.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier8.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath8, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VIII #\n",
    "\n",
    "model_dropout8 = classifier8.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VIII - the best trained on the validation set #\n",
    "\n",
    "Model_VIII = load_model(model_filepath8)\n",
    "Model_VIII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIIIa filepath #\n",
    "\n",
    "model_filepath8a = project_dir + '/models/model_checkpoint8a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model VIIIa (single Dropout) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier8a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier8a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier8a.add(Flatten())\n",
    "classifier8a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(Dropout(0.5))\n",
    "classifier8a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier8a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier8a.compile(optimizer=optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath8a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training VIIIa #\n",
    "\n",
    "model_dropout8a = classifier8a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy VIIIa - the best trained on the validation set #\n",
    "\n",
    "Model_VIIIa = load_model(model_filepath8a)\n",
    "Model_VIIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IX filepath #\n",
    "\n",
    "model_filepath9 = project_dir + '/models/model_checkpoint9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IX (single Dropout - second dense layer) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier9 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier9.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9.add(Flatten())\n",
    "classifier9.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9.add(Dropout(0.5))\n",
    "classifier9.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier9.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath9, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IX #\n",
    "\n",
    "model_dropout9 = classifier9.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IX - the best trained on the validation set #\n",
    "\n",
    "Model_IX = load_model(model_filepath9)\n",
    "Model_IX.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IXa filepath #\n",
    "\n",
    "model_filepath9a = project_dir + '/models/model_checkpoint9a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IXa (single Dropout - second dense layer) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier9a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier9a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier9a.add(Flatten())\n",
    "classifier9a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier9a.add(Dropout(0.5))\n",
    "classifier9a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier9a.compile(optimizer=optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath9a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training IXa #\n",
    "\n",
    "model_dropout9a = classifier9a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy IXa - the best trained on the validation set #\n",
    "\n",
    "Model_IXa = load_model(model_filepath9a)\n",
    "Model_IXa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model X filepath #\n",
    "\n",
    "model_filepath10 = project_dir + '/models/model_checkpoint10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model X (max norm on all layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier10 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier10.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10.add(Flatten())\n",
    "classifier10.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier10.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath10, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training X #\n",
    "\n",
    "model_max_norm10 = classifier10.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy X - the best trained on the validation set #\n",
    "\n",
    "Model_X = load_model(model_filepath10)\n",
    "Model_X.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Xa filepath #\n",
    "\n",
    "model_filepath10a = project_dir + '/models/model_checkpoint10a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Xa (max norm on all layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier10a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier10a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier10a.add(Flatten())\n",
    "classifier10a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier10a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier10a.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath10a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training X #\n",
    "\n",
    "model_max_norm10a = classifier10a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy Xa - the best trained on the validation set #\n",
    "\n",
    "Model_Xa = load_model(model_filepath10a)\n",
    "Model_Xa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XI filepath #\n",
    "\n",
    "model_filepath11 = project_dir + '/models/model_checkpoint11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XI (max norm only on dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier11 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier11.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11.add(Flatten())\n",
    "classifier11.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier11.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier11.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier11.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath11, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XI #\n",
    "\n",
    "model_max_norm11 = classifier11.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XI - the best trained on the validation set #\n",
    "\n",
    "Model_XI = load_model(model_filepath11)\n",
    "Model_XI.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIa filepath #\n",
    "\n",
    "model_filepath11a = project_dir + '/models/model_checkpoint11a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIa (max norm only on dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier11a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier11a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11a.add(Flatten())\n",
    "classifier11a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier11a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier11a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier11a.compile(optimizer=optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath11a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIa #\n",
    "\n",
    "model_max_norm11a = classifier11a.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIa - the best trained on the validation set #\n",
    "\n",
    "Model_XIa = load_model(model_filepath11a)\n",
    "Model_XIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIb filepath #\n",
    "\n",
    "model_filepath11b = project_dir + '/models/model_checkpoint11b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIb (max norm only on dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier11b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier11b.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11b.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11b.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier11b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier11b.add(Flatten())\n",
    "classifier11b.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier11b.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier11b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier11b.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath11b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIb #\n",
    "\n",
    "model_max_norm11b = classifier11b.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIb - the best trained on the validation set #\n",
    "\n",
    "Model_XIb = load_model(model_filepath11b)\n",
    "Model_XIb.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XII filepath #\n",
    "\n",
    "model_filepath12 = project_dir + '/models/model_checkpoint12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XII (max norm only on convolutional layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier12 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier12.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12.add(Flatten())\n",
    "classifier12.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier12.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier12.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier12.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath12, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier12.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XII #\n",
    "\n",
    "model_max_norm12 = classifier12.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XII - the best trained on the validation set #\n",
    "\n",
    "Model_XII = load_model(model_filepath12)\n",
    "Model_XII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIIa filepath #\n",
    "\n",
    "model_filepath12a = project_dir + '/models/model_checkpoint12a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIIa (max norm only on convolutional layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier12a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier12a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier12a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier12a.add(Flatten())\n",
    "classifier12a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier12a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier12a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier12a.compile(optimizer=optimziers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath12a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIIa #\n",
    "\n",
    "model_max_norm12a = classifier12a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIIa - the best trained on the validation set #\n",
    "\n",
    "Model_XIIa = load_model(model_filepath12a)\n",
    "Model_XIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIII filepath #\n",
    "\n",
    "model_filepath13 = project_dir + '/models/model_checkpoint13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIII (max norm only on dense layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier13 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier13.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13.add(Flatten())\n",
    "classifier13.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier13.add(Dropout(0.5))\n",
    "classifier13.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier13.add(Dropout(0.5))\n",
    "classifier13.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier13.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath13, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier13.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIII #\n",
    "\n",
    "model_max_norm13 = classifier13.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIII - the best trained on the validation set #\n",
    "\n",
    "Model_XIII = load_model(model_filepath13)\n",
    "Model_XIII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIIIa filepath #\n",
    "\n",
    "model_filepath13a = project_dir + '/models/model_checkpoint13a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIIIa (max norm only on dense layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier13a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier13a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier13a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier13a.add(Flatten())\n",
    "classifier13a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier13a.add(Dropout(0.5))\n",
    "classifier13a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier13a.add(Dropout(0.5))\n",
    "classifier13a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier13a.compile(optimizer=optimizers.Adam(0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath13a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIIIa #\n",
    "\n",
    "model_max_norm13a = classifier13a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIIIa - the best trained on the validation set #\n",
    "\n",
    "Model_XIIIa = load_model(model_filepath13a)\n",
    "Model_XIIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIV filepath #\n",
    "\n",
    "model_filepath14 = project_dir + '/models/model_checkpoint14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIV (max norm only on convolutional layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier14 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier14.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14.add(Flatten())\n",
    "classifier14.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier14.add(Dropout(0.5))\n",
    "classifier14.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier14.add(Dropout(0.5))\n",
    "classifier14.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier14.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath14, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier14.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIV #\n",
    "\n",
    "model_max_norm14 = classifier14.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIV - the best trained on the validation set #\n",
    "\n",
    "Model_XIV = load_model(model_filepath14)\n",
    "Model_XIV.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIVa filepath #\n",
    "\n",
    "model_filepath14a = project_dir + '/models/model_checkpoint14a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIVa (max norm only on convolutional layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier14a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier14a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier14a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier14a.add(Flatten())\n",
    "classifier14a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier14a.add(Dropout(0.5))\n",
    "classifier14a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier14a.add(Dropout(0.5))\n",
    "classifier14a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier14a.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath14a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIVa #\n",
    "\n",
    "model_max_norm14a = classifier14a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIVa - the best trained on the validation set #\n",
    "\n",
    "Model_XIVa = load_model(model_filepath14a)\n",
    "Model_XIVa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XV filepath #\n",
    "\n",
    "model_filepath15 = project_dir + '/models/model_checkpoint15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XV (max norm on all layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier15 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier15.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15.add(Flatten())\n",
    "classifier15.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(Dropout(0.5))\n",
    "classifier15.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15.add(Dropout(0.5))\n",
    "classifier15.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier15.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath15, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier15.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XV #\n",
    "\n",
    "model_max_norm15 = classifier15.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XV - the best trained on the validation set #\n",
    "\n",
    "Model_XV = load_model(model_filepath15)\n",
    "Model_XV.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVa filepath #\n",
    "\n",
    "model_filepath15a = project_dir + '/models/model_checkpoint15a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVa (max norm on all layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier15a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier15a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15a.add(Flatten())\n",
    "classifier15a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(Dropout(0.5))\n",
    "classifier15a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15a.add(Dropout(0.5))\n",
    "classifier15a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier15a.compile(optimizer=optimizers.Adam(0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath15a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVa #\n",
    "\n",
    "model_max_norm15a = classifier15a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XV - the best trained on the validation set #\n",
    "\n",
    "Model_XVa = load_model(model_filepath15a)\n",
    "Model_XVa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVb filepath #\n",
    "\n",
    "model_filepath15b = project_dir + '/models/model_checkpoint15b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVb (max norm on all layers + Dropout on two dense layers) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier15b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier15b.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15b.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15b.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier15b.add(Flatten())\n",
    "classifier15b.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(Dropout(0.5))\n",
    "classifier15b.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier15b.add(Dropout(0.5))\n",
    "classifier15b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier15b.compile(optimizer=optimizers.Adam(0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath15b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVb #\n",
    "\n",
    "model_max_norm15b = classifier15b.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVb - the best trained on the validation set #\n",
    "\n",
    "Model_XVb = load_model(model_filepath15b)\n",
    "Model_XVb.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVI filepath #\n",
    "\n",
    "model_filepath16 = project_dir + '/models/model_checkpoint16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVI (max_norm only on dense layers + Dropout only on first dense layer) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier16 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier16.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16.add(Flatten())\n",
    "classifier16.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier16.add(Dropout(0.5))\n",
    "classifier16.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier16.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier16.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath16, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVI #\n",
    "\n",
    "model_max_norm16 = classifier16.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVI - the best trained on the validation set #\n",
    "\n",
    "Model_XVI = load_model(model_filepath16)\n",
    "Model_XVI.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIa filepath #\n",
    "\n",
    "model_filepath16a = project_dir + '/models/model_checkpoint16a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIa (max_norm only on dense layers + Dropout only on first dense layer) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier16a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier16a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier16a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier16a.add(Flatten())\n",
    "classifier16a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier16a.add(Dropout(0.5))\n",
    "classifier16a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier16a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier16a.compile(optimizer=optimizers.Adam(0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath16a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVIa #\n",
    "\n",
    "model_max_norm16a = classifier16a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVIa - the best trained on the validation set #\n",
    "\n",
    "Model_XVIa = load_model(model_filepath16a)\n",
    "Model_XVIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVII filepath #\n",
    "\n",
    "model_filepath17 = project_dir + '/models/model_checkpoint17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVII (max_norm only on dense layers + Dropout only on second dense layer) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier17 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier17.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17.add(Flatten())\n",
    "classifier17.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier17.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier17.add(Dropout(0.5))\n",
    "classifier17.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier17.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath17, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier17.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVII #\n",
    "\n",
    "model_max_norm17 = classifier17.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVII - the best trained on the validation set #\n",
    "\n",
    "Model_XVII = load_model(model_filepath17)\n",
    "Model_XVII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIIa filepath #\n",
    "\n",
    "model_filepath17a = project_dir + '/models/model_checkpoint17a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIIa (max_norm only on dense layers + Dropout only on second dense layer) #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier17a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier17a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier17a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier17a.add(Flatten())\n",
    "classifier17a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier17a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "classifier17a.add(Dropout(0.5))\n",
    "classifier17a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_constraint=max_norm(3)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier17a.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath17a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVIIa #\n",
    "\n",
    "model_max_norm17a = classifier17a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVIIa - the best trained on the validation set #\n",
    "\n",
    "Model_XVIIa = load_model(model_filepath17a)\n",
    "Model_XVIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIII filepath #\n",
    "\n",
    "model_filepath18 = project_dir + '/models/model_checkpoint18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIII Batch Normalization on all layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier18 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier18.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18.add(Flatten())\n",
    "classifier18.add(Dense(units=256, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(Dense(units=128, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier18.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath18, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier18.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVIII #\n",
    "\n",
    "model_batch_norm18 = classifier18.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVIII - the best trained on the validation set #\n",
    "\n",
    "Model_XVIII = load_model(model_filepath18)\n",
    "Model_XVIII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIIIa filepath #\n",
    "\n",
    "model_filepath18a = project_dir + '/models/model_checkpoint18a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIIIa Batch Normalization on all layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier18a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier18a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18a.add(Flatten())\n",
    "classifier18a.add(Dense(units=256, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(Dense(units=128, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier18a.compile(optimizer=optimizers(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath18a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVIIIa #\n",
    "\n",
    "model_batch_norm18a = classifier18a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVIIIa - the best trained on the validation set #\n",
    "\n",
    "Model_XVIIIa = load_model(model_filepath18a)\n",
    "Model_XVIIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIIIb filepath #\n",
    "\n",
    "model_filepath18b = project_dir + '/models/model_checkpoint18b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XVIIIb Batch Normalization on all layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier18b = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier18b.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18b.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18b.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18b.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier18b.add(Flatten())\n",
    "classifier18b.add(Dense(units=256, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(Dense(units=128, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier18b.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier18b.compile(optimizer=optimizers(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath18b, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XVIIIb #\n",
    "\n",
    "model_batch_norm18b = classifier18b.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XVIIIb - the best trained on the validation set #\n",
    "\n",
    "Model_XVIIIb = load_model(model_filepath18b)\n",
    "Model_XVIIIb.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIX filepath #\n",
    "\n",
    "model_filepath19 = project_dir + '/models/model_checkpoint19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIX Batch Normalization only on convolutional layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier19 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier19.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19.add(Flatten())\n",
    "classifier19.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier19.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier19.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier19.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath19, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIX #\n",
    "\n",
    "model_batch_norm19 = classifier19.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIX - the best trained on the validation set #\n",
    "\n",
    "Model_XIX = load_model(model_filepath19)\n",
    "Model_XIX.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIXa filepath #\n",
    "\n",
    "model_filepath19a = project_dir + '/models/model_checkpoint19a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XIXa Batch Normalization only on convolutional layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier19a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier19a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=False,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier19a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier19a.add(Flatten())\n",
    "classifier19a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier19a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier19a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier19a.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath19a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XIXa #\n",
    "\n",
    "model_batch_norm19a = classifier19a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XIXa - the best trained on the validation set #\n",
    "\n",
    "Model_XIXa = load_model(model_filepath19a)\n",
    "Model_XIXa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XX filepath #\n",
    "\n",
    "model_filepath20 = project_dir + '/models/model_checkpoint20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XX Batch Normalization only on dense layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier20 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier20.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20.add(Flatten())\n",
    "classifier20.add(Dense(units=256, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier20.add(Dense(units=128, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier20.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier20.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath20, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier20.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XX #\n",
    "\n",
    "model_batch_norm20 = classifier20.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XX - the best trained on the validation set #\n",
    "\n",
    "Model_XX = load_model(model_filepath20)\n",
    "Model_XX.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXa filepath #\n",
    "\n",
    "model_filepath20a = project_dir + '/models/model_checkpoint20a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXa Batch Normalization only on dense layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier20a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier20a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier20a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier20a.add(Flatten())\n",
    "classifier20a.add(Dense(units=256, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier20a.add(Dense(units=128, activation='relu', use_bias=False, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "BatchNormalization()\n",
    "classifier20a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier20a.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath20a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXa #\n",
    "\n",
    "model_batch_norm20a = classifier20a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXa - the best trained on the validation set #\n",
    "\n",
    "Model_XXa = load_model(model_filepath20a)\n",
    "Model_XXa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXI filepath #\n",
    "\n",
    "model_filepath21 = project_dir + '/models/model_checkpoint21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXI - L2 regularization on all layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier21 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier21.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21.add(Flatten())\n",
    "classifier21.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier21.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier21.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier21.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath21, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier21.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXI #\n",
    "\n",
    "model_L2_21 = classifier21.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXI - the best trained on the validation set #\n",
    "\n",
    "Model_XXI = load_model(model_filepath21)\n",
    "Model_XXI.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIa filepath #\n",
    "\n",
    "model_filepath21a = project_dir + '/models/model_checkpoint21a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIa - L2 regularization on all layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier21a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier21a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier21a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier21a.add(Flatten())\n",
    "classifier21a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier21a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier21a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier21a.compile(optimizer=optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath21a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXIa #\n",
    "\n",
    "model_L2_21a = classifier21a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXIa - the best trained on the validation set #\n",
    "\n",
    "Model_XXIa = load_model(model_filepath21a)\n",
    "Model_XXIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXII filepath #\n",
    "\n",
    "model_filepath22 = project_dir + '/models/model_checkpoint22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXII - L2 regularization only on convolutional layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier22 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier22.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22.add(Flatten())\n",
    "classifier22.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier22.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier22.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier22.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath22, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier22.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXII #\n",
    "\n",
    "model_L2_22 = classifier22.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXII - the best trained on the validation set #\n",
    "\n",
    "Model_XXII = load_model(model_filepath22)\n",
    "Model_XXII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIIa filepath #\n",
    "\n",
    "model_filepath22a = project_dir + '/models/model_checkpoint22a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIIa - L2 regularization only on convolutional layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier22a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier22a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.005)))\n",
    "classifier22a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier22a.add(Flatten())\n",
    "classifier22a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier22a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier22a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier22a.compile(optimizer=optimizers.Adam(0.05), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath22a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXIIa #\n",
    "\n",
    "model_L2_2a = classifier22a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXIIa - the best trained on the validation set #\n",
    "\n",
    "Model_XXIIa = load_model(model_filepath22a)\n",
    "Model_XXIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIII filepath #\n",
    "\n",
    "model_filepath23 = project_dir + '/models/model_checkpoint23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIII - L2 regularization only on dense layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier23 = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier23.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23.add(Flatten())\n",
    "classifier23.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier23.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier23.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier23.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath23, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier23.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXIII #\n",
    "\n",
    "model_L2_23 = classifier23.fit(train_X, train_Y, batch_size=64, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXIII - the best trained on the validation set #\n",
    "\n",
    "Model_XXIII = load_model(model_filepath23)\n",
    "Model_XXIII.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIIIa filepath #\n",
    "\n",
    "model_filepath23a = project_dir + '/models/model_checkpoint23a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XXIIIa - L2 regularization only on dense layers #\n",
    "\n",
    "# Model initialization #\n",
    "\n",
    "classifier23a = Sequential()\n",
    "\n",
    "# Model architecture #\n",
    "\n",
    "classifier23a.add(Conv2D(16, (3, 3), input_shape=(256, 256, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23a.add(Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23a.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23a.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', use_bias=True,\n",
    "                      bias_initializer='zeros', kernel_initializer='glorot_uniform'))\n",
    "classifier23a.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "classifier23a.add(Flatten())\n",
    "classifier23a.add(Dense(units=256, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier23a.add(Dense(units=128, activation='relu', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                      kernel_regularizer=regularizers.l2(0.01)))\n",
    "classifier23a.add(Dense(units=5, activation='softmax', use_bias=True, bias_initializer='zeros', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Model compilation #\n",
    "\n",
    "classifier23a.compile(optimizer=optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialization of model training early stop and saving the best model for the time being #\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "CheckPoint = callbacks.ModelCheckpoint(filepath=model_filepath23a, monitor='val_accuracy', mode='max', verbose=1,\n",
    "                                       save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training XXIIIa #\n",
    "\n",
    "model_L2_23a = classifier23a.fit(train_X, train_Y, batch_size=32, epochs=100, validation_data=(validation_X, validation_Y), shuffle=True, \n",
    "                            callbacks=[earlystopping, CheckPoint], class_weight={0: 1.14511252939201, 1: 1.50242397531952, 2: 1, 3: 1.72345803842265, 4: 7.04338842975207})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classification accuracy XXIIIa - the best trained on the validation set #\n",
    "\n",
    "Model_XXIIIa = load_model(model_filepath23a)\n",
    "Model_XXIIIa.evaluate(x=validation_X, y=validation_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy of the best selected model on the test set #\n",
    "\n",
    "Model_IV.evaluate(x=test_X, y=test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of accuracy metrics during the learning process on the training and validation set #\n",
    "\n",
    "fig8_dir = project_dir + '/Figures/figure8.jpg'\n",
    "accuracy = model_plain4.history['accuracy']\n",
    "val_accuracy = model_plain4.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(epochs, accuracy, 'b', linewidth=4, label='Dokładność na zbiorze treningowym')\n",
    "ax.plot(epochs, val_accuracy, 'r', linewidth=4, label='Dokładność na zbiorze walidacyjnym')\n",
    "plt.fill_between(epochs, val_accuracy, accuracy, where=np.array(accuracy) > np.array(val_accuracy), color='yellow', label='Overfitting')\n",
    "ax.scatter(val_accuracy.index(max(val_accuracy)), max(val_accuracy), s=150, c='black')\n",
    "ax.annotate('val_accuracy max', xy=(39, 0.7), xytext=(30, 0.5), arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "ax.set_xlabel('Epoka', fontsize=12)\n",
    "ax.set_ylabel('Dokładność klasyfikacji', fontsize=12)\n",
    "ax.set_title('Dokładność klasyfikacji względem epoki uczenia modelu', fontsize=13)\n",
    "ax.legend()\n",
    "plt.savefig(fig8_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set #\n",
    "\n",
    "predicted_classes = Model_IV.predict(test_X)\n",
    "predicted_classes = np.argmax(np.round(predicted_classes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of prediction quality for age categories in the test set #\n",
    "\n",
    "classes = np.unique(test_Y)\n",
    "target_names =['Kategoria {}'.format(i) for i in classes]\n",
    "print(classification_report(test_Y, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sketching the multidimensional confusion matrix of the prediction for age categories #\n",
    "\n",
    "fig9_dir = project_dir + '/Figures/figure9.jpg'\n",
    "cm = confusion_matrix(test_Y, predicted_classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "ax.set_xlabel('Kategoria prognozowana przez model', fontsize=12)\n",
    "ax.set_ylabel('Prawdziwa kategoria', fontsize=12)\n",
    "plt.title('Wielowymiarowy confusion matrix', fontsize=16)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.savefig(fig9_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists of model misclassifications for appropriate age categories #\n",
    "\n",
    "prediction_error_0_class = []\n",
    "prediction_error_1_class = []\n",
    "prediction_error_2_class = []\n",
    "prediction_error_3_class = []\n",
    "prediction_error_4_class = []\n",
    "for i in range(len(test_Y)):\n",
    "    if test_Y[i] != predicted_classes[i]:\n",
    "        if test_Y[i] == 0:\n",
    "            prediction_error_0_class.append((test_X[i], test_Y[i], predicted_classes[i]))\n",
    "        elif test_Y[i] == 1:\n",
    "            prediction_error_1_class.append((test_X[i], test_Y[i], predicted_classes[i]))\n",
    "        elif test_Y[i] == 2:\n",
    "            prediction_error_2_class.append((test_X[i], test_Y[i], predicted_classes[i]))\n",
    "        elif test_Y[i] == 3:\n",
    "            prediction_error_3_class.append((test_X[i], test_Y[i], predicted_classes[i]))\n",
    "        elif test_Y[i] == 4:\n",
    "            prediction_error_4_class.append((test_X[i], test_Y[i], predicted_classes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification errors for category 0 # \n",
    "\n",
    "fig10_dir = project_dir + '/Figures/figure10.jpg'\n",
    "plt.figure(figsize=[18, 7])\n",
    "\n",
    "Category_0 = list(zip ([1, 2, 3, 4, 5], [prediction_error_0_class[75], prediction_error_0_class[40],\n",
    "                                            prediction_error_0_class[67], prediction_error_0_class[17]]))\n",
    "\n",
    "for i in Category_0:\n",
    "    plt.subplot(1, 5, i[0])\n",
    "    plt.imshow(i[1][0])\n",
    "    plt.title('Prognozowana kategoria:' + str(i[1][2]))\n",
    "plt.savefig(fig10_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification errors for category 1 # \n",
    "\n",
    "fig12_dir = project_dir + '/Figures/figure12.jpg'\n",
    "plt.figure(figsize=[18, 7])\n",
    "\n",
    "Category_1 = list(zip ([1, 2, 3, 4, 5], [prediction_error_1_class[82], prediction_error_1_class[21],\n",
    "                                            prediction_error_1_class[64], prediction_error_1_class[47]]))\n",
    "\n",
    "for i in Category_1:\n",
    "    plt.subplot(1, 5, i[0])\n",
    "    plt.imshow(i[1][0])\n",
    "    plt.title('Prognozowana kategoria:' + str(i[1][2]))\n",
    "plt.savefig(fig12_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification errors for category 2 # \n",
    "\n",
    "fig14_dir = project_dir + '/Figures/figure14.jpg'\n",
    "plt.figure(figsize=[18, 7])\n",
    "\n",
    "Category_2 = list(zip ([1, 2, 3, 4, 5], [prediction_error_2_class[32], prediction_error_2_class[4],\n",
    "                                            prediction_error_2_class[87], prediction_error_2_class[39]]))\n",
    "\n",
    "for i in Category_2:\n",
    "    plt.subplot(1, 5, i[0])\n",
    "    plt.imshow(i[1][0])\n",
    "    plt.title('Prognozowana kategoria:' + str(i[1][2]))\n",
    "plt.savefig(fig14_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification errors for category 3 # \n",
    "\n",
    "fig16_dir = project_dir + '/Figures/figure16.jpg'\n",
    "plt.figure(figsize=[18, 7])\n",
    "\n",
    "Category_3 = list(zip ([1, 2, 3, 4, 5], [prediction_error_3_class[18], prediction_error_3_class[48],\n",
    "                                            prediction_error_3_class[1], prediction_error_3_class[43]]))\n",
    "\n",
    "for i in Category_3:\n",
    "    plt.subplot(1, 5, i[0])\n",
    "    plt.imshow(i[1][0])\n",
    "    plt.title('Prognozowana kategoria:' + str(i[1][2]))\n",
    "plt.savefig(fig16_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification errors for category 4 # \n",
    "\n",
    "fig18_dir = project_dir + '/Figures/figure18.jpg'\n",
    "plt.figure(figsize=[18, 7])\n",
    "\n",
    "Category_4 = list(zip ([1, 2, 3, 4, 5], [prediction_error_4_class[53], prediction_error_4_class[25],\n",
    "                                            prediction_error_4_class[1], prediction_error_4_class[87]]))\n",
    "\n",
    "for i in Category_4:\n",
    "    plt.subplot(1, 5, i[0])\n",
    "    plt.imshow(i[1][0])\n",
    "    plt.title('Prognozowana kategoria:' + str(i[1][2]))\n",
    "plt.savefig(fig18_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
